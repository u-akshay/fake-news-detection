{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake News Detection\n#### Akshay U\nIts a Machine Learning program to find Fake news by training this system with Naive Bayes.\n\nWe have two datasets. `True.csv` and `Fake.csv`. <br>\nTrue.csv contains only true news and Fake.csv contains only fake news."},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Location of Dataset\nhttps://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"},{"metadata":{},"cell_type":"markdown","source":"### Import data and Cleaning"},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake_df = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the dataset, its shape and basic info. "},{"metadata":{},"cell_type":"markdown","source":"Details of True.csv"},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df['title'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Details of Fake.csv"},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df['title'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check NaN\nCheck if any null oe NaN values in the dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We identified that this dataset is clean without NaN values. Also, we understand that we need only the `text` attribute for predicting the output. <br>\nSo we should divide the dataset columns for input and output."},{"metadata":{},"cell_type":"markdown","source":"Add a new column as `label` for store the news as REAL or FAKE.\n\nThen concatinate the two dataframe to one for training."},{"metadata":{"trusted":false},"cell_type":"code","source":"true_df['label'] = \"Real\"\ntrue_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fake_df['label'] = \"Fake\"\nfake_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.concat([true_df,fake_df])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inputs"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df['text']\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Corresponding outputs"},{"metadata":{"trusted":false},"cell_type":"code","source":"y = df['label']\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=99)\nprint(\"Training set - Features: \", X_train.shape, \"  Target: \", y_train.shape)\nprint(\"Testing set  - Features: \", X_test.shape, \"  Target: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the split"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Extraction\n<p style='text-align: justify'>Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process. Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original data set.</p>\n\nInitialize a CountVectorizer with stop_words = 'english'. \n\nThen use **fit()** and store the result to a variable for make ***joblib*** file."},{"metadata":{"trusted":false},"cell_type":"code","source":"vect = CountVectorizer(stop_words='english')\nvectorizer = vect.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_transformed = vect.transform(X_train)\nX_test_transformed = vect.transform(X_test)\nprint(\"New Transformed...\")\nprint(\"Training set - Features: \", X_train_transformed.shape, \"  Target: \", y_train.shape)\nprint(\"Testing set  - Features: \", X_test_transformed.shape, \"  Target: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling - Naive Bayes"},{"metadata":{"trusted":false},"cell_type":"code","source":"def print_metrics(labels, preds):\n    print(\"Precision Score\\t: {}\".format(precision_score(labels, preds, average='weighted')))\n    print(\"Recall Score\\t: {}\".format(recall_score(labels, preds, average='weighted')))\n    print(\"Accuracy Score\\t: {}\".format(accuracy_score(labels, preds)))\n    print(\"F1 Score\\t: {}\".format(f1_score(labels, preds, average='weighted')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mnb = MultinomialNB()\nmnb.fit(X_train_transformed,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction and Accuracy"},{"metadata":{"trusted":false},"cell_type":"code","source":"prediction = mnb.predict(X_test_transformed)\nprint_metrics(prediction, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(prediction, y_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mnb.classes_)\ndisp.plot() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train with full set of data -- 100% of data and store it for future prediction\nWe found the accuracy of this machine with 80% training data.<br>For the future prediction, we can train the machine with 100% dataset, which may increase the accuracy."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train_transformed = vect.transform(X)\nnaive = mnb.fit(X_train_transformed,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Joblib File\nOur dataset is little bit big, hence we are using ***joblib*** instead ***pickle***. Joblib file work similar to pickle file. And this file is using for future prediction and helps to avoid training the machine over again."},{"metadata":{"trusted":false},"cell_type":"code","source":"joblib.dump(naive,\"naive.joblib\")\njoblib.dump(vectorizer,\"vectoriszer.joblib\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}